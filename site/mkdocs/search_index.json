{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mkdocs",
            "text": "For full documentation visit  mkdocs.org .",
            "title": "Welcome to MkDocs"
        },
        {
            "location": "/#commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Commands"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Project layout"
        },
        {
            "location": "/rviz_desktop_report/",
            "text": "% A next generation desktop infrastructure for HPC\n% Jonas Lindemann;Anders Follin\n% 2016-01-26\n\n\nIntroduction\n\n\nDuring the 2013-2015 Lunarc has been operating a desktop infrastructure on the Alarik resource, to evaluate if this could complement the existing terminal-based environment. This environment have been continously been in production since mid 2013 and been a great success among the users. We currently have approximately between 30-50 active users in this environment.\n\n\nA prototype system was procured to test and develop ideas for a more rich and scalable SNIC desktop architecture. The prototype was installed in the beginning of 2015 and will be the blueprint for the upcoming Aurora desktop infrastructure.\n\n\nThe Alarik Desktop infrastructure\n\n\nThe desktop infrastructure deployed on Alarik consists of a front-end server with a Thinlinc server, 2 back-end nodes acting as desktop servers (Thinlinc agents). The Thinlinc server will assign users to one of the back-end servers depending on the current load of the back-end server. \n\n\nThe back-end servers only support 2D graphics or emulated 3D graphics. To support applications requiring hardware accelerated graphics, 2 additional servers with NVIDIA Quadro 5000 cards provide this functionality.\n\n\nHardware graphics support is implemented using the VirtualGL protocol and is setup for specific applications provided throug a customised menu in desktop environment. Running such an application will connect to the backend graphics server using a special protocol, vglconnect, which will stream the accelerated application from the back-end server.\n\n\n\n\nLessons learned\n\n\nIn general the implemented infrastructure have worked very well and have had no major issues. However, some lessons have been learnt during the use of this setup.\n\n\n\n\nThe desktop infrastructure can scale by adding server, but a session is assigned to a specific back-end server and shares the resources with other users on the same resource.\n\n\nResource limits are not easily enforced. It is possible for users with large visualisation needs to overallocate memory and the user experience of other users on the same node will suffer severely.\n\n\nIt is difficult to provide users with exclusive access to dedicated server for running graphical applications.\n\n\nVirtualGL has to be configure on a per application basis and access to the graphics card is shared, reducing the performance depending on the number of users using the graphics card at the same time.\n\n\nSupporting the desktop system is not the same as supporting an HPC resource. Tools for monitoring desktop sessions needs to be developed to provide a smooth user experience.\n\n\nExisting authentication solutions must be taken into account when implementing a desktop infrastructure. The One Time Password system used at Lunarc needed to be adjusted to be able to used with Thinlinc to support the re-use of OTP-tokens.\n\n\nThere is a reliance on commercial software, such as Thinlinc, which can be a risk as well as an advantage. \n\n\n\n\nThe SNIC ET Prototype system\n\n\nTo be able to try different desktop solutions and configurations a prototype system was procured during end of 2014. The procured hardware was chosen in a way that we could evaluate many of the availble solutions as possible. \n\n\nThe final hardware consisted of:\n\n\n\n\nXenServer with NVIDIA Grid K1/K2 cards \n\n\n48 GB Memory\n\n\n2 x 2620v3 Xeon Processors at 2.4 GHz\n\n\n1 x NVIDIA Grid K1 card\n\n\n1 x NVIDIA Grid K2 card\n\n\nStorage array with 6 x 1 TB 2.5 inch drives\n\n\nServer with consumer grade NVIDIA cards\n\n\n64 GB Memory\n\n\n2 x 2620v3 Xeon Processors at 2.4 GHz\n\n\n2 x NVIDIA Grid GTX 980\n\n\nServer with Xeon processor with built-in graphics\n\n\n32 GB Memory\n\n\n1 x Xeon E3-1268L\n\n\nInfiniband switch\n\n\nGigabit switch\n\n\n\n\nVirtual environment for sharing GPU:s\n\n\nAn interesting development is the ability for virtual environments to provide access to hardware based graphics acceleration to GPU-nodes. Sharing can be done using 2 methods. In the first method, a part of a GPU is shared with a virtual machine. This is often called virtual GPU or vGPU. Similar to the concept of virtual CPU or vCPU in standard virtual environments. The method is currently not well supported for Linux based virtual machines. In the second method, an entire GPU is shared with a virtual machine. The address space of the GPU is passed-through to the virtual machine. The method is often called pass-through GPU.\n\n\nFor Windows based virtual machines the vGPU concept works well and the RemoteFX protocol and client can be used to access the accelerated desktop. Performance is good and the limiting factor here is the network-bandwidth to the client application. The problem with this approach is to find a way of integrating Windows support in a SNIC desktop infrastructure. This issue is covered more in the following sections.\n\n\nPassing through GPU:s to Linux based virtual machines works well using XenServer as long as a generic Linux is selected when creating the virtual machine. If a supported Linux distribution is selected pass-through is disabled. To be able to take advantage of the GPU the NVIDIA driver has to be installed on the virtual machine and configured for desktop use. As the pass-through GPU does not provide a display in a normal sense. Access to acceleration must be provided through VirtualGL or similar methods of capturing the image stream from the GPU. We have succesfully tested CentOS 7 based virtual machines with pass-through GPU.\n\n\nUsing the installed K1/K2 cards from NVIDIA the prototype can provide 6 (4+2) pass-through GPU:s to Linux based virtual machines. For moderate needs the K2 card can be replaced with a K1 card and the soluion can provide 8 Linux based virtual machines with accelerated graphics. If vGPU gets better Linux support a single XenServer can provide up to 32 virtual machines with accelerated graphics.\n\n\nXeon processors with built-in graphics\n\n\nAnother interesting development is the introduction of built-in graphics in Xeon server processors. Previously Intel support for built-in graphics was limited to desktop machines and laptops, making it difficult to provide a solution that can be used in a data center or HPC context. \n\n\nIntel now have several Xeon server processors with built-in graphics with the Xeon E3-1200 family. One of the servers in the prototype uses a E3-1268L processor with built-in graphics. These processors are often provided in a small formfactors, enabling high-density solutions with supporting accelerated graphics. \n\n\nThe graphics performance of these processors are limited to low-end graphics acceleration, but could be a solution for moderate graphics need. \n\n\nConsumer grade graphics solution\n\n\nFor enterprise and high-end graphics, NVIDIA provides the Quadro series of graphics cards. These are expensive cards often tailored for enterprise applications. An idea we discussed in this project, was to evaluate the use of consumer grade graphics cards such as the NVIDIA GTX 980 to provide high-end graphics performance to a lower cost. A machine with 2 GTX 980 cards where installed in a standard rack-mountable workstation. The performance is very good and they can provide an alternative to the Quadro cards. However, the problem with the consumer cards is that they are designed to be actively cooled with a built-in fan, limiting its use in high-density deployment.  \n\n\nRemote desktop solutions\n\n\nUnix and Linux have since many years been able to provide remote desktop services using the X11 protocol. However, the X11 protocol requires the user to install a X11 server on the client system, which can be very complicated and not well supported on all platforms. Also, the protocol itself is not inherintly secure and to use it securely it has to be tunneled or used through a VPN, increasing the complexity of using this solution. \n\n\nA a more platform independent solution is Virtual Network Computer or VNC. This technology uses the Remote Frame Buffer protocol or RFB. The technology relays mouse and keyboard event to a remote computer and transfers updated portions of the frame buffer over the network. VNC is open source and has spawned several projects providing different implementations of the protocol. By default the VNC implementation uses a unsecure authentication mechanism, but many of the VNC projects add an additional security layer either by tunneling through SSH or implementing its own security layer.\n\n\nThere also exists several proprietary solutions as well implementing their own protocols and authentication. In this project we have evaluated some of these protocols to see what benefits and limitations they provide.  \n\n\nThinlinc\n\n\nThinLinc is a VNC implementation from a Swedish company, Cendio. The ThinLinc solution include both a native client for Mac OS X, Linux and Windows and server side components. \n\n\nThe server side part of ThinLinc can be used to implement a single server solution as well as scalable solution over several servers (agents). The main server load balances desktop sessions over all agents depending on the current load of the agent server.\n\n\nOne of the biggest benefits of using ThinLinc is their support of native, easy to use, clients for all major platforms. With a simple one-click installer the client can be installed on all major platforms.\n\n\nNice DCV\n\n\nDesktop Cloud Visualisation, DCV, from NICE is a proprietary solution and protocol for providing remote desktop services. DCV supports hardware acceleration using NVIDIA GRID technologies as well as fully supporting hardware acceleration under Windows as well as Linux. \n\n\nThe server side is built on VNC with custom additions for supporting hardware accelerated graphics. The DCV server can also operate in different modes. \n\n\n\n\nPass-through mode supporting a single VM with a passthrough GPU\n\n\nXenServer 6.x support vGPUs\n\n\nStandalone server with GPU.\n\n\n\n\nThe benefits of NICE DCV is the support of multiple OS application servers, enabling users to run hardware accelerated Windows and Linux applications. When evaluating DCV, it was not clear how it's security framework can integrate in a HPC Linux environment. Does it support PAM-modules and integration of external authentication protcols? Pricing of the product is also quite high compared to the Thinlinc solution.\n\n\nThe NICE DCV solution was evaluated on XenServer 6.5 using the NVIDIA K1 and K2 cards. Installation was easy and it supported vGPU sharing under Windows.\n\n\nMicrosoft Remote Desktop / RemoteFX\n\n\nMicrosoft have had remote desktop services built-in to the professional and enterprise offerings for a long time, with their Remote Desktop Protocol or RDP. Recently they support hardware accelererated graphics through RDP using a technology called RemoteFX. \n\n\nThe RDP protocol in its non-accelerated form is supported from a Linux client using the rdesktop application. The RemoteFX protocol is however not yet supported. Using RDP, 2D Windows applications can be provided to a Linux based desktop environment using the rdesktop tool.\n\n\nThe RemoteFX protocol has been evaluated on the XenServer 6.5 using the NVIDIA K1 an K2 cards and works vGPUs with a special NVIDIA driver. Performance is good enough to run high-end visualisation software and games.\n\n\nXenApp / XenDesktop\n\n\nThe RDP protocol was originally developed by Citrix for Microsoft as their remote desktop product Terminal Server. Citrix has continued their development of RDP and this protocol is what is used in their remote desktop/application solutions XenDesktop and XenApp. \n\n\nXenDesktop provides a desktop environment and XenApp is a special derivative of the XenDesktop that only provide remote access to specific graphical applications without the desktop.\n\n\nThe XenDesktop and XenApp solutions can both work with hardware accelerated graphics natively or through vGPU on a hypervisor. Currently the major platform is Windows, but there is work on providing support for Linux based application and dekstop virtualisation.\n\n\nIt was decided not to evaluate this solution as its support for Linux is not yet developed. \n\n\nPrototype Desktop Infrastructure\n\n\nThe high-level goals of a next generation desktop infrastructure:\n\n\n\n\nSupport allocation of desktop sessions through a resource management system such as SLURM.\n\n\nProvide a scalable solution that can be extended based on user demands.\n\n\nCreate a easy to use system for users to allocate custom desktop sessions without using command line tools. \n\n\nSupport additional OS environments such as Windows through a single unified desktop.\n\n\n\n\nPrototype setup\n\n\nTo be able to evaluate the different scenarios the prototype setup was implemented as a standard HPC cluster with commonly found services such as:\n\n\n\n\nResource manager (rviz-intel) - SLURM for job management\n\n\nLDAP server (ldap1) - for user and group authentication\n\n\nOTP server (otp1) - for two-factor authentication (PhenixID server)\n\n\n\n\nThe prototype also has 3 networks:\n\n\n\n\nExternal network - available on rviz-intel.\n\n\nInternal gigabit network - available on rviz-intel and all nodes.\n\n\nInfiniband network - available on all rviz-intel and all nodes.\n\n\nAdmin network - for handling hardware interfaces.\n\n\n\n\nXenServer 6.5 Hypervisor\n\n\nTo be able to evaluate how hypervisors can be used to provide hardware accelerated graphics to virtual machines, the XenServer 6.5 Hypervisor was installed on the server with the NVIDIA GRID K1/K2 cards. The hypervisor was also used to provide additional services to the prototype such as LDAP and OTP services. \n\n\nXenServer 6.5 is able to provide GPU:s either as vGPU:s for Windows OS or using pass-trhough on Linux based distributions. To evaluate Linux support a virtual machine was created and assigned a NVIDIA K1 or K2 card. The machine was then installed with CentOS 7 as this is the OS that is most likely to be the target platform of upcoming SNIC resources.\n\n\n\n\n\n\nInstalling NVIDIA drivers on virtual machine",
            "title": "Report"
        },
        {
            "location": "/rviz_desktop_report/#introduction",
            "text": "During the 2013-2015 Lunarc has been operating a desktop infrastructure on the Alarik resource, to evaluate if this could complement the existing terminal-based environment. This environment have been continously been in production since mid 2013 and been a great success among the users. We currently have approximately between 30-50 active users in this environment.  A prototype system was procured to test and develop ideas for a more rich and scalable SNIC desktop architecture. The prototype was installed in the beginning of 2015 and will be the blueprint for the upcoming Aurora desktop infrastructure.",
            "title": "Introduction"
        },
        {
            "location": "/rviz_desktop_report/#the-alarik-desktop-infrastructure",
            "text": "The desktop infrastructure deployed on Alarik consists of a front-end server with a Thinlinc server, 2 back-end nodes acting as desktop servers (Thinlinc agents). The Thinlinc server will assign users to one of the back-end servers depending on the current load of the back-end server.   The back-end servers only support 2D graphics or emulated 3D graphics. To support applications requiring hardware accelerated graphics, 2 additional servers with NVIDIA Quadro 5000 cards provide this functionality.  Hardware graphics support is implemented using the VirtualGL protocol and is setup for specific applications provided throug a customised menu in desktop environment. Running such an application will connect to the backend graphics server using a special protocol, vglconnect, which will stream the accelerated application from the back-end server.",
            "title": "The Alarik Desktop infrastructure"
        },
        {
            "location": "/rviz_desktop_report/#lessons-learned",
            "text": "In general the implemented infrastructure have worked very well and have had no major issues. However, some lessons have been learnt during the use of this setup.   The desktop infrastructure can scale by adding server, but a session is assigned to a specific back-end server and shares the resources with other users on the same resource.  Resource limits are not easily enforced. It is possible for users with large visualisation needs to overallocate memory and the user experience of other users on the same node will suffer severely.  It is difficult to provide users with exclusive access to dedicated server for running graphical applications.  VirtualGL has to be configure on a per application basis and access to the graphics card is shared, reducing the performance depending on the number of users using the graphics card at the same time.  Supporting the desktop system is not the same as supporting an HPC resource. Tools for monitoring desktop sessions needs to be developed to provide a smooth user experience.  Existing authentication solutions must be taken into account when implementing a desktop infrastructure. The One Time Password system used at Lunarc needed to be adjusted to be able to used with Thinlinc to support the re-use of OTP-tokens.  There is a reliance on commercial software, such as Thinlinc, which can be a risk as well as an advantage.",
            "title": "Lessons learned"
        },
        {
            "location": "/rviz_desktop_report/#the-snic-et-prototype-system",
            "text": "To be able to try different desktop solutions and configurations a prototype system was procured during end of 2014. The procured hardware was chosen in a way that we could evaluate many of the availble solutions as possible.   The final hardware consisted of:   XenServer with NVIDIA Grid K1/K2 cards   48 GB Memory  2 x 2620v3 Xeon Processors at 2.4 GHz  1 x NVIDIA Grid K1 card  1 x NVIDIA Grid K2 card  Storage array with 6 x 1 TB 2.5 inch drives  Server with consumer grade NVIDIA cards  64 GB Memory  2 x 2620v3 Xeon Processors at 2.4 GHz  2 x NVIDIA Grid GTX 980  Server with Xeon processor with built-in graphics  32 GB Memory  1 x Xeon E3-1268L  Infiniband switch  Gigabit switch",
            "title": "The SNIC ET Prototype system"
        },
        {
            "location": "/rviz_desktop_report/#virtual-environment-for-sharing-gpus",
            "text": "An interesting development is the ability for virtual environments to provide access to hardware based graphics acceleration to GPU-nodes. Sharing can be done using 2 methods. In the first method, a part of a GPU is shared with a virtual machine. This is often called virtual GPU or vGPU. Similar to the concept of virtual CPU or vCPU in standard virtual environments. The method is currently not well supported for Linux based virtual machines. In the second method, an entire GPU is shared with a virtual machine. The address space of the GPU is passed-through to the virtual machine. The method is often called pass-through GPU.  For Windows based virtual machines the vGPU concept works well and the RemoteFX protocol and client can be used to access the accelerated desktop. Performance is good and the limiting factor here is the network-bandwidth to the client application. The problem with this approach is to find a way of integrating Windows support in a SNIC desktop infrastructure. This issue is covered more in the following sections.  Passing through GPU:s to Linux based virtual machines works well using XenServer as long as a generic Linux is selected when creating the virtual machine. If a supported Linux distribution is selected pass-through is disabled. To be able to take advantage of the GPU the NVIDIA driver has to be installed on the virtual machine and configured for desktop use. As the pass-through GPU does not provide a display in a normal sense. Access to acceleration must be provided through VirtualGL or similar methods of capturing the image stream from the GPU. We have succesfully tested CentOS 7 based virtual machines with pass-through GPU.  Using the installed K1/K2 cards from NVIDIA the prototype can provide 6 (4+2) pass-through GPU:s to Linux based virtual machines. For moderate needs the K2 card can be replaced with a K1 card and the soluion can provide 8 Linux based virtual machines with accelerated graphics. If vGPU gets better Linux support a single XenServer can provide up to 32 virtual machines with accelerated graphics.",
            "title": "Virtual environment for sharing GPU:s"
        },
        {
            "location": "/rviz_desktop_report/#xeon-processors-with-built-in-graphics",
            "text": "Another interesting development is the introduction of built-in graphics in Xeon server processors. Previously Intel support for built-in graphics was limited to desktop machines and laptops, making it difficult to provide a solution that can be used in a data center or HPC context.   Intel now have several Xeon server processors with built-in graphics with the Xeon E3-1200 family. One of the servers in the prototype uses a E3-1268L processor with built-in graphics. These processors are often provided in a small formfactors, enabling high-density solutions with supporting accelerated graphics.   The graphics performance of these processors are limited to low-end graphics acceleration, but could be a solution for moderate graphics need.",
            "title": "Xeon processors with built-in graphics"
        },
        {
            "location": "/rviz_desktop_report/#consumer-grade-graphics-solution",
            "text": "For enterprise and high-end graphics, NVIDIA provides the Quadro series of graphics cards. These are expensive cards often tailored for enterprise applications. An idea we discussed in this project, was to evaluate the use of consumer grade graphics cards such as the NVIDIA GTX 980 to provide high-end graphics performance to a lower cost. A machine with 2 GTX 980 cards where installed in a standard rack-mountable workstation. The performance is very good and they can provide an alternative to the Quadro cards. However, the problem with the consumer cards is that they are designed to be actively cooled with a built-in fan, limiting its use in high-density deployment.",
            "title": "Consumer grade graphics solution"
        },
        {
            "location": "/rviz_desktop_report/#remote-desktop-solutions",
            "text": "Unix and Linux have since many years been able to provide remote desktop services using the X11 protocol. However, the X11 protocol requires the user to install a X11 server on the client system, which can be very complicated and not well supported on all platforms. Also, the protocol itself is not inherintly secure and to use it securely it has to be tunneled or used through a VPN, increasing the complexity of using this solution.   A a more platform independent solution is Virtual Network Computer or VNC. This technology uses the Remote Frame Buffer protocol or RFB. The technology relays mouse and keyboard event to a remote computer and transfers updated portions of the frame buffer over the network. VNC is open source and has spawned several projects providing different implementations of the protocol. By default the VNC implementation uses a unsecure authentication mechanism, but many of the VNC projects add an additional security layer either by tunneling through SSH or implementing its own security layer.  There also exists several proprietary solutions as well implementing their own protocols and authentication. In this project we have evaluated some of these protocols to see what benefits and limitations they provide.",
            "title": "Remote desktop solutions"
        },
        {
            "location": "/rviz_desktop_report/#thinlinc",
            "text": "ThinLinc is a VNC implementation from a Swedish company, Cendio. The ThinLinc solution include both a native client for Mac OS X, Linux and Windows and server side components.   The server side part of ThinLinc can be used to implement a single server solution as well as scalable solution over several servers (agents). The main server load balances desktop sessions over all agents depending on the current load of the agent server.  One of the biggest benefits of using ThinLinc is their support of native, easy to use, clients for all major platforms. With a simple one-click installer the client can be installed on all major platforms.",
            "title": "Thinlinc"
        },
        {
            "location": "/rviz_desktop_report/#nice-dcv",
            "text": "Desktop Cloud Visualisation, DCV, from NICE is a proprietary solution and protocol for providing remote desktop services. DCV supports hardware acceleration using NVIDIA GRID technologies as well as fully supporting hardware acceleration under Windows as well as Linux.   The server side is built on VNC with custom additions for supporting hardware accelerated graphics. The DCV server can also operate in different modes.    Pass-through mode supporting a single VM with a passthrough GPU  XenServer 6.x support vGPUs  Standalone server with GPU.   The benefits of NICE DCV is the support of multiple OS application servers, enabling users to run hardware accelerated Windows and Linux applications. When evaluating DCV, it was not clear how it's security framework can integrate in a HPC Linux environment. Does it support PAM-modules and integration of external authentication protcols? Pricing of the product is also quite high compared to the Thinlinc solution.  The NICE DCV solution was evaluated on XenServer 6.5 using the NVIDIA K1 and K2 cards. Installation was easy and it supported vGPU sharing under Windows.",
            "title": "Nice DCV"
        },
        {
            "location": "/rviz_desktop_report/#microsoft-remote-desktop-remotefx",
            "text": "Microsoft have had remote desktop services built-in to the professional and enterprise offerings for a long time, with their Remote Desktop Protocol or RDP. Recently they support hardware accelererated graphics through RDP using a technology called RemoteFX.   The RDP protocol in its non-accelerated form is supported from a Linux client using the rdesktop application. The RemoteFX protocol is however not yet supported. Using RDP, 2D Windows applications can be provided to a Linux based desktop environment using the rdesktop tool.  The RemoteFX protocol has been evaluated on the XenServer 6.5 using the NVIDIA K1 an K2 cards and works vGPUs with a special NVIDIA driver. Performance is good enough to run high-end visualisation software and games.",
            "title": "Microsoft Remote Desktop / RemoteFX"
        },
        {
            "location": "/rviz_desktop_report/#xenapp-xendesktop",
            "text": "The RDP protocol was originally developed by Citrix for Microsoft as their remote desktop product Terminal Server. Citrix has continued their development of RDP and this protocol is what is used in their remote desktop/application solutions XenDesktop and XenApp.   XenDesktop provides a desktop environment and XenApp is a special derivative of the XenDesktop that only provide remote access to specific graphical applications without the desktop.  The XenDesktop and XenApp solutions can both work with hardware accelerated graphics natively or through vGPU on a hypervisor. Currently the major platform is Windows, but there is work on providing support for Linux based application and dekstop virtualisation.  It was decided not to evaluate this solution as its support for Linux is not yet developed.",
            "title": "XenApp / XenDesktop"
        },
        {
            "location": "/rviz_desktop_report/#prototype-desktop-infrastructure",
            "text": "The high-level goals of a next generation desktop infrastructure:   Support allocation of desktop sessions through a resource management system such as SLURM.  Provide a scalable solution that can be extended based on user demands.  Create a easy to use system for users to allocate custom desktop sessions without using command line tools.   Support additional OS environments such as Windows through a single unified desktop.",
            "title": "Prototype Desktop Infrastructure"
        },
        {
            "location": "/rviz_desktop_report/#prototype-setup",
            "text": "To be able to evaluate the different scenarios the prototype setup was implemented as a standard HPC cluster with commonly found services such as:   Resource manager (rviz-intel) - SLURM for job management  LDAP server (ldap1) - for user and group authentication  OTP server (otp1) - for two-factor authentication (PhenixID server)   The prototype also has 3 networks:   External network - available on rviz-intel.  Internal gigabit network - available on rviz-intel and all nodes.  Infiniband network - available on all rviz-intel and all nodes.  Admin network - for handling hardware interfaces.",
            "title": "Prototype setup"
        },
        {
            "location": "/rviz_desktop_report/#xenserver-65-hypervisor",
            "text": "To be able to evaluate how hypervisors can be used to provide hardware accelerated graphics to virtual machines, the XenServer 6.5 Hypervisor was installed on the server with the NVIDIA GRID K1/K2 cards. The hypervisor was also used to provide additional services to the prototype such as LDAP and OTP services.   XenServer 6.5 is able to provide GPU:s either as vGPU:s for Windows OS or using pass-trhough on Linux based distributions. To evaluate Linux support a virtual machine was created and assigned a NVIDIA K1 or K2 card. The machine was then installed with CentOS 7 as this is the OS that is most likely to be the target platform of upcoming SNIC resources.",
            "title": "XenServer 6.5 Hypervisor"
        },
        {
            "location": "/rviz_desktop_report/#installing-nvidia-drivers-on-virtual-machine",
            "text": "",
            "title": "Installing NVIDIA drivers on virtual machine"
        }
    ]
}